#!/usr/bin/env python3
import os
import glob
import re
import pandas as pd

def parse_job_log(job_log_path):
    """
    Parse the job log file generated by SLURM.
    Expected lines are of the form:
      Key: Value
    """
    data = {}
    try:
        with open(job_log_path, 'r') as f:
            for line in f:
                if ':' in line:
                    key, value = line.split(':', 1)
                    data[key.strip()] = value.strip()
    except Exception as e:
        print(f"Error reading {job_log_path}: {e}")
    return data

def parse_namd_out(namd_out_path):
    """
    Parse the NAMD output file to extract performance and I/O info.
    This function uses regular expressions to extract values. You may need to adjust
    the patterns to your file's exact format.
    """
    data = {}
    try:
        with open(namd_out_path, 'r') as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading {namd_out_path}: {e}")
        return data

    # Define regex patterns for each required field.
    patterns = {
        # yet to define
    }
    for key, pattern in patterns.items():
        match = re.search(pattern, content)
        data[key] = match.group(1) if match else None
    return data

def extract_type_from_filename(filename):
    """
    Extract the 'Type of Prod' from the NAMD output filename.
    It searches for known keywords in the filename.
    """
    types = ['vanilla', 'imdv3', 'fileio', 'streaming']
    fname = filename.lower()
    for t in types:
        if t in fname:
            return t
    return "N/A"

def extract_run_type_and_purpose(job_log_data):
    """
    Extract additional run type and purpose info from the job log data.
    You can customize this function based on how these fields are recorded in your log.
    """
    # For example, assume the job log may contain:
    #   Type of Run: prod
    #   Purpose of Run: performance
    run_type = job_log_data.get("Type of Run", "prod")
    purpose = job_log_data.get("Purpose of Run", "performance")
    return run_type, purpose

def main():
    output_dir = "output"
    rows = []

    # Look for folders starting with 'run-' inside the output directory.
    run_dirs = glob.glob(os.path.join(output_dir, "run-*"))
    if not run_dirs:
        print("No run-* folders found in the output directory.")
        return

    for run_dir in run_dirs:
        # Assume the job log file is one folder up from the run folder.
        parent_dir = os.path.dirname(run_dir)
        job_log_files = glob.glob(os.path.join(parent_dir, "job_info_slurm_*.log"))
        if job_log_files:
            job_log_data = parse_job_log(job_log_files[0])
        else:
            print(f"No job log file found for {run_dir}")
            job_log_data = {}

        # Find the NAMD output file (step5_*.out) inside the run folder.
        namd_out_files = glob.glob(os.path.join(run_dir, "step5_*.out"))
        if not namd_out_files:
            print(f"No NAMD out file found in {run_dir}")
            continue
        # If there are multiple, take the first.
        namd_out_file = namd_out_files[0]
        namd_out_data = parse_namd_out(namd_out_file)

        # Extract the type of production from the NAMD output filename.
        type_of_prod = extract_type_from_filename(os.path.basename(namd_out_file))
        # Extract Type of Run and Purpose from job log (or default values)
        run_type, purpose = extract_run_type_and_purpose(job_log_data)

        # Build the row dictionary using keys from both the job log and NAMD output.
        row = {
            "Date and Time Job submitted": job_log_data.get("Job Submitted", None),
            "Date and Time Job Ran": job_log_data.get("Job Started", None),
            "Cluster Name": job_log_data.get("Cluster Name", None),
            "Requested Node Name": job_log_data.get("Node Name List", None),
            "Request Number of GPUs": job_log_data.get("Requested number of GPUs", None),
            "GPU type": job_log_data.get("Requested GPU Type", None),
            "GPU ID": job_log_data.get("GPU IDs", None),
            "Requested Number of CPUs": job_log_data.get("Requested number of CPUs", None),
            "CPU ID list": job_log_data.get("CPU IDs", None),
            "Type of Run": run_type,
            "Type of Prod": type_of_prod,
            "Purpose of Run": purpose,
            "Number of GPUs used": job_log_data.get("Requested number of GPUs", None),  # Adjust if actual usage is recorded elsewhere.
            "Number of CPUs used": job_log_data.get("Requested number of CPUs", None),
            "Frequency of *.out file I/O": namd_out_data.get("Frequency of *.out file I/O", None),
            "Frequency of File I/O": namd_out_data.get("Frequency of File I/O", None),
            "position freq": namd_out_data.get("position freq", None),
            "velocity freq": namd_out_data.get("velocity freq", None),
            "force freq": namd_out_data.get("force freq", None),
            "box dimensions freq": namd_out_data.get("box dimensions freq", None),
            "Frequency of streaming I/O": namd_out_data.get("Frequency of streaming I/O", None),
            "Time Step (fs)": namd_out_data.get("Time Step (fs)", None),
            "Number of Steps": namd_out_data.get("Number of Steps", None),
            "Total time of run": namd_out_data.get("Total time of run", None),
            "Average performance (ns/day)": namd_out_data.get("Average performance (ns/day)", None),
            "Standard deviation of performance": namd_out_data.get("Standard deviation of performance", None)
        }
        rows.append(row)

    # Create a DataFrame with one row per unique run.
    df = pd.DataFrame(rows)
    # Optionally, save the DataFrame to a CSV file.
    df.to_csv("runs_summary.csv", index=False)
    print("Summary saved to runs_summary.csv")

if __name__ == "__main__":
    main()
